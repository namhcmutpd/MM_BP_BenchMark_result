import os
import time
import tracemalloc
import numpy as np
import csv
from pathlib import Path
from datetime import datetime

# Import c√°c module c·ªßa b·∫°n
from src.PetriNet import PetriNet
from src.BDD import bdd_reachable
from src.BFS import bfs_reachable
from src.DFS import dfs_reachable

def measure_performance(algo_name, func, pn):
    """
    H√†m ph·ª• tr·ª£ ƒë·ªÉ ƒëo th·ªùi gian v√† b·ªô nh·ªõ c·ªßa m·ªôt thu·∫≠t to√°n.
    """
    print(f"    {algo_name:<20}", end=" ", flush=True)
    
    # 1. Reset v√† B·∫Øt ƒë·∫ßu ƒëo b·ªô nh·ªõ
    tracemalloc.stop() # Stop n·∫øu ƒëang ch·∫°y
    tracemalloc.start()
    
    # 2. B·∫Øt ƒë·∫ßu b·∫•m gi·ªù
    start_time = time.perf_counter()
    
    # 3. Ch·∫°y thu·∫≠t to√°n
    try:
        if algo_name == "BDD Symbolic":
            # BDD tr·∫£ v·ªÅ (bdd_object, count)
            _, count = func(pn)
        else:
            # BFS/DFS tr·∫£ v·ªÅ Set[Tuple]
            visited_set = func(pn)
            count = len(visited_set)
            
    except Exception as e:
        print(f"FAILED! ({e})")
        return None
        
    # 4. D·ª´ng b·∫•m gi·ªù
    end_time = time.perf_counter()
    
    # 5. L·∫•y th√¥ng s·ªë b·ªô nh·ªõ (peak)
    _, peak_mem = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    
    # 6. T√≠nh to√°n
    elapsed_time_ms = (end_time - start_time) * 1000
    peak_mem_mb = peak_mem / (1024 * 1024)
    
    print("‚úì")
    return {
        "name": algo_name,
        "count": count,
        "time_ms": elapsed_time_ms,
        "memory_mb": peak_mem_mb
    }

def test_single_file(filename):
    """
    Test m·ªôt file Petri Net v·ªõi c·∫£ 3 thu·∫≠t to√°n.
    Tr·∫£ v·ªÅ dictionary ch·ª©a k·∫øt qu·∫£.
    """
    print(f"\nTesting: {filename}")
    
    if not os.path.exists(filename):
        print(f"  ‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file {filename}.")
        return None

    # ƒê·ªçc PetriNet
    print(f"  Reading file...", end=" ", flush=True)
    try:
        pn = PetriNet.from_pnml(filename)
        print("‚úì")
    except Exception as e:
        print(f"FAILED! ({e})")
        return None
        
    places_count = len(pn.place_names)
    trans_count = len(pn.trans_names)
    print(f"  Net Info: {places_count} places, {trans_count} transitions.")

    # Ch·∫°y l·∫ßn l∆∞·ª£t 3 thu·∫≠t to√°n
    results = []
    
    # --- A. BDD Symbolic ---
    res_bdd = measure_performance("BDD Symbolic", bdd_reachable, pn)
    if res_bdd: results.append(res_bdd)
    
    # --- B. BFS Explicit ---
    res_bfs = measure_performance("BFS Explicit", bfs_reachable, pn)
    if res_bfs: results.append(res_bfs)
    
    # --- C. DFS Explicit ---
    res_dfs = measure_performance("DFS Explicit", dfs_reachable, pn)
    if res_dfs: results.append(res_dfs)

    # Ki·ªÉm tra t√≠nh ƒë√∫ng ƒë·∫Øn (Consistency Check)
    consistency_ok = True
    if len(results) == 3:
        count_bdd = results[0]['count']
        count_bfs = results[1]['count']
        count_dfs = results[2]['count']
        
        if count_bdd == count_bfs == count_dfs:
            print(f"  ‚úÖ Consistency check: PASS (All found {count_bdd} states)")
        else:
            print(f"  ‚ùå Consistency check: FAIL")
            print(f"     BDD: {count_bdd}, BFS: {count_bfs}, DFS: {count_dfs}")
            consistency_ok = False

    return {
        "filename": filename,
        "places": places_count,
        "transitions": trans_count,
        "results": results,
        "consistency_ok": consistency_ok
    }

def find_pnml_files():
    """
    T√¨m t·∫•t c·∫£ c√°c file .pnml trong th∆∞ m·ª•c hi·ªán t·∫°i.
    """
    pnml_files = []
    for file in os.listdir('.'):
        if file.endswith('.pnml'):
            pnml_files.append(file)
    return sorted(pnml_files)

def generate_summary_report(all_results, output_file="benchmark_results.txt"):
    """
    T·∫°o file t·ªïng h·ª£p k·∫øt qu·∫£ so s√°nh hi·ªáu su·∫•t.
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("="*100 + "\n")
        f.write("PETRI NET REACHABILITY ANALYSIS - BENCHMARK REPORT\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("="*100 + "\n\n")

        # T√≥m t·∫Øt t·ª´ng file
        for file_result in all_results:
            if file_result is None:
                continue
            
            f.write(f"\nFile: {file_result['filename']}\n")
            f.write(f"  Places: {file_result['places']}, Transitions: {file_result['transitions']}\n")
            f.write("-" * 100 + "\n")
            f.write(f"  {'ALGORITHM':<20} | {'STATES':<12} | {'TIME (ms)':<20} | {'MEMORY (MB)':<20}\n")
            f.write("-" * 100 + "\n")
            
            for res in file_result['results']:
                f.write(f"  {res['name']:<20} | {res['count']:<12} | {res['time_ms']:<20.4f} | {res['memory_mb']:<20.6f}\n")
            
            consistency_status = "‚úÖ PASS" if file_result['consistency_ok'] else "‚ùå FAIL"
            f.write(f"\n  Consistency Check: {consistency_status}\n")

        # B·∫£ng so s√°nh t·ªïng h·ª£p
        f.write("\n\n" + "="*100 + "\n")
        f.write("PERFORMANCE COMPARISON SUMMARY\n")
        f.write("="*100 + "\n\n")
        
        f.write(f"{'FILE':<35} | {'BDD Time':<20} | {'BFS Time':<20} | {'DFS Time':<20}\n")
        f.write("-" * 100 + "\n")
        
        for file_result in all_results:
            if file_result is None or len(file_result['results']) < 3:
                continue
            
            filename = file_result['filename']
            bdd_time = file_result['results'][0]['time_ms']
            bfs_time = file_result['results'][1]['time_ms']
            dfs_time = file_result['results'][2]['time_ms']
            
            f.write(f"{filename:<35} | {bdd_time:<20.4f} | {bfs_time:<20.4f} | {dfs_time:<20.4f}\n")

        # Th·ªëng k√™ t·ªïng h·ª£p
        f.write("\n\n" + "="*100 + "\n")
        f.write("AGGREGATE STATISTICS\n")
        f.write("="*100 + "\n\n")
        
        bdd_times = []
        bfs_times = []
        dfs_times = []
        bdd_mems = []
        bfs_mems = []
        dfs_mems = []
        
        for file_result in all_results:
            if file_result is None or len(file_result['results']) < 3:
                continue
            
            bdd_times.append(file_result['results'][0]['time_ms'])
            bfs_times.append(file_result['results'][1]['time_ms'])
            dfs_times.append(file_result['results'][2]['time_ms'])
            bdd_mems.append(file_result['results'][0]['memory_mb'])
            bfs_mems.append(file_result['results'][1]['memory_mb'])
            dfs_mems.append(file_result['results'][2]['memory_mb'])
        
        f.write("EXECUTION TIME (ms):\n")
        f.write("-" * 60 + "\n")
        f.write(f"  BDD Symbolic:\n")
        f.write(f"    Mean:   {np.mean(bdd_times):.4f} ms\n")
        f.write(f"    Median: {np.median(bdd_times):.4f} ms\n")
        f.write(f"    Std:    {np.std(bdd_times):.4f} ms\n")
        f.write(f"    Min:    {np.min(bdd_times):.4f} ms\n")
        f.write(f"    Max:    {np.max(bdd_times):.4f} ms\n\n")
        
        f.write(f"  BFS Explicit:\n")
        f.write(f"    Mean:   {np.mean(bfs_times):.4f} ms\n")
        f.write(f"    Median: {np.median(bfs_times):.4f} ms\n")
        f.write(f"    Std:    {np.std(bfs_times):.4f} ms\n")
        f.write(f"    Min:    {np.min(bfs_times):.4f} ms\n")
        f.write(f"    Max:    {np.max(bfs_times):.4f} ms\n\n")
        
        f.write(f"  DFS Explicit:\n")
        f.write(f"    Mean:   {np.mean(dfs_times):.4f} ms\n")
        f.write(f"    Median: {np.median(dfs_times):.4f} ms\n")
        f.write(f"    Std:    {np.std(dfs_times):.4f} ms\n")
        f.write(f"    Min:    {np.min(dfs_times):.4f} ms\n")
        f.write(f"    Max:    {np.max(dfs_times):.4f} ms\n\n")
        
        f.write("MEMORY USAGE (MB):\n")
        f.write("-" * 60 + "\n")
        f.write(f"  BDD Symbolic:\n")
        f.write(f"    Mean:   {np.mean(bdd_mems):.6f} MB\n")
        f.write(f"    Median: {np.median(bdd_mems):.6f} MB\n")
        f.write(f"    Std:    {np.std(bdd_mems):.6f} MB\n")
        f.write(f"    Min:    {np.min(bdd_mems):.6f} MB\n")
        f.write(f"    Max:    {np.max(bdd_mems):.6f} MB\n\n")
        
        f.write(f"  BFS Explicit:\n")
        f.write(f"    Mean:   {np.mean(bfs_mems):.6f} MB\n")
        f.write(f"    Median: {np.median(bfs_mems):.6f} MB\n")
        f.write(f"    Std:    {np.std(bfs_mems):.6f} MB\n")
        f.write(f"    Min:    {np.min(bfs_mems):.6f} MB\n")
        f.write(f"    Max:    {np.max(bfs_mems):.6f} MB\n\n")
        
        f.write(f"  DFS Explicit:\n")
        f.write(f"    Mean:   {np.mean(dfs_mems):.6f} MB\n")
        f.write(f"    Median: {np.median(dfs_mems):.6f} MB\n")
        f.write(f"    Std:    {np.std(dfs_mems):.6f} MB\n")
        f.write(f"    Min:    {np.min(dfs_mems):.6f} MB\n")
        f.write(f"    Max:    {np.max(dfs_mems):.6f} MB\n\n")

        # So s√°nh hi·ªáu su·∫•t
        f.write("\n" + "="*100 + "\n")
        f.write("PERFORMANCE COMPARISON\n")
        f.write("="*100 + "\n\n")
        
        avg_bdd = np.mean(bdd_times)
        avg_bfs = np.mean(bfs_times)
        avg_dfs = np.mean(dfs_times)
        
        f.write(f"Average Execution Time:\n")
        f.write(f"  BDD:  {avg_bdd:.4f} ms\n")
        f.write(f"  BFS:  {avg_bfs:.4f} ms\n")
        f.write(f"  DFS:  {avg_dfs:.4f} ms\n\n")
        
        fastest = min([("BDD", avg_bdd), ("BFS", avg_bfs), ("DFS", avg_dfs)], key=lambda x: x[1])
        f.write(f"Fastest Algorithm: {fastest[0]} ({fastest[1]:.4f} ms)\n\n")
        
        f.write(f"Speed Comparison (relative to fastest):\n")
        f.write(f"  BDD: {avg_bdd/fastest[1]:.2f}x\n")
        f.write(f"  BFS: {avg_bfs/fastest[1]:.2f}x\n")
        f.write(f"  DFS: {avg_dfs/fastest[1]:.2f}x\n\n")
        
        f.write("="*100 + "\n")

def generate_csv_report(all_results, output_file="benchmark_results.csv"):
    """
    T·∫°o file CSV ƒë·ªÉ d·ªÖ ph√¢n t√≠ch d·ªØ li·ªáu.
    """
    with open(output_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['Filename', 'Places', 'Transitions', 'Algorithm', 'States Found', 'Time (ms)', 'Memory (MB)', 'Consistency OK'])
        
        for file_result in all_results:
            if file_result is None:
                continue
            
            for res in file_result['results']:
                writer.writerow([
                    file_result['filename'],
                    file_result['places'],
                    file_result['transitions'],
                    res['name'],
                    res['count'],
                    f"{res['time_ms']:.4f}",
                    f"{res['memory_mb']:.6f}",
                    "Yes" if file_result['consistency_ok'] else "No"
                ])

def main():
    print("\n" + "="*100)
    print("PETRI NET REACHABILITY ANALYSIS - BENCHMARK FOR ALL FILES")
    print("="*100)
    
    # T√¨m t·∫•t c·∫£ c√°c file .pnml
    pnml_files = find_pnml_files()
    
    if not pnml_files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file .pnml n√†o trong th∆∞ m·ª•c hi·ªán t·∫°i.")
        return
    
    print(f"\n‚úì Found {len(pnml_files)} PNML files:\n")
    for i, f in enumerate(pnml_files, 1):
        print(f"  {i}. {f}")
    
    print("\n" + "-"*100)
    
    # Test t·ª´ng file
    all_results = []
    for filename in pnml_files:
        result = test_single_file(filename)
        all_results.append(result)
    
    print("\n" + "-"*100)
    
    # T·∫°o file t·ªïng h·ª£p
    print("\nüìä Generating summary reports...")
    generate_summary_report(all_results, "benchmark_results.txt")
    generate_csv_report(all_results, "benchmark_results.csv")
    print(f"  ‚úì Text report: benchmark_results.txt")
    print(f"  ‚úì CSV report: benchmark_results.csv")
    
    print("\n" + "="*100)
    print("BENCHMARK COMPLETE!")
    print("="*100 + "\n")

if __name__ == "__main__":
    main()
